{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<font color='red' size='5px'/> Data Preprocessing<font/>","metadata":{"id":"2c8a2e61"}},{"cell_type":"markdown","source":"<font color='blue' size='5px'/> Introduction<font/>","metadata":{"id":"143b3b39"}},{"cell_type":"markdown","source":"#  Overview ","metadata":{"id":"00d8c7aa"}},{"cell_type":"markdown","source":"We have decleared the dataset to be [Dataset](https://iplab.dmi.unict.it/Deepfakechallenge/). Now we want to choose how to perform preprocessing on this dataset. We've decided that we are going to split into two teams each team will be working with different methedology. The first will use Keras and the other will use Albumentations libarary","metadata":{"id":"657c3e4c"}},{"cell_type":"markdown","source":"We want to use Data augmentation techniques to increase the size and diversity of a dataset by applying transformations to the existing data. This helps to prevent overfitting and improves the generalization of the model.","metadata":{"id":"a3d4ffca"}},{"cell_type":"markdown","source":"#  Literature Review","metadata":{"id":"082056e6"}},{"cell_type":"markdown","source":"## 1 Models Used To detect DeepFake & Datasets","metadata":{"id":"ce450de4"}},{"cell_type":"markdown","source":"Models:\n1. XceptionNet: A deep learning model that is commonly used for deepfake detection, as it has shown high accuracy in identifying manipulated videos.\n\n2. EfficientNet: Another deep learning model that is known for its high efficiency and accuracy in deepfake detection.\n\n3. Capsule-Forensics: A model that uses a capsule network to extract features and identify manipulated videos based on the presence of anomalous patterns.\n\n4. MesoNet: A deep learning model that uses mesoscopic features to identify manipulated videos.\n\n5. FFD-Net: A model that uses a frequency-based approach to identify the presence of deepfake artifacts in manipulated videos.\n\n6. Two-Stream Convolutional Neural Network (CNN): A model that uses a combination of spatial and temporal information to detect deepfake videos.\n\nDatasets:\n1. FaceForensics++: A dataset that contains a collection of videos that have been manipulated using different deepfake techniques. The dataset is commonly used for training and evaluating deepfake detection models.\n\n2. DFDNet: A deep learning model that uses a fusion of deep and handcrafted features to detect manipulated videos.","metadata":{"id":"3a7c1443"}},{"cell_type":"code","source":"from IPython.display import Image","metadata":{"id":"32dbfb7f","execution":{"iopub.status.busy":"2023-03-25T02:25:06.768308Z","iopub.execute_input":"2023-03-25T02:25:06.769371Z","iopub.status.idle":"2023-03-25T02:25:06.774968Z","shell.execute_reply.started":"2023-03-25T02:25:06.769327Z","shell.execute_reply":"2023-03-25T02:25:06.773685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Image('The-ameliorated-XceptionNet-architecture.png')","metadata":{"id":"712e7868","outputId":"666024eb-1b18-4548-8fde-a7dcfcbb2491","execution":{"iopub.status.busy":"2023-03-27T13:38:03.523777Z","iopub.execute_input":"2023-03-27T13:38:03.524708Z","iopub.status.idle":"2023-03-27T13:38:03.530628Z","shell.execute_reply.started":"2023-03-27T13:38:03.524671Z","shell.execute_reply":"2023-03-27T13:38:03.529408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Image('EfficientNet3D-B0-Architecture.jpg')","metadata":{"id":"f6912781","outputId":"a3240a8f-9c48-46b1-8dab-56a158ef3567","execution":{"iopub.status.busy":"2023-03-27T13:38:03.532332Z","iopub.execute_input":"2023-03-27T13:38:03.532669Z","iopub.status.idle":"2023-03-27T13:38:03.540449Z","shell.execute_reply.started":"2023-03-27T13:38:03.532640Z","shell.execute_reply":"2023-03-27T13:38:03.539075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2 The Face Deepfake Detection Challenge Paper","metadata":{"id":"3501fa03"}},{"cell_type":"markdown","source":"## 2.1 Introduction","metadata":{"id":"0c57e8a8"}},{"cell_type":"markdown","source":"1. Data Collection: The authors collected a large dataset of real and fake face images from various sources, including publicly available datasets and images generated using deepfake techniques.\n\n2. Data Preparation: The dataset was split into training, validation, and testing sets. The authors also applied various data augmentation techniques to increase the size of the training set and improve the generalization of the model.\n\n3. Baseline Model: The authors developed a baseline deep learning model for face detection using a combination of convolutional neural networks (CNNs) and recurrent neural networks (RNNs). The model was trained on the training set and evaluated on the validation set.\n\n4. Challenge: The authors organized a challenge in which participants were asked to develop their own deepfake detection models and submit their results for evaluation. The challenge provided a platform for researchers to compare their methods and identify the most effective techniques.\n\n5. Evaluation: The authors evaluated the performance of the participating models using various metrics, including precision, recall, and F1 score. They also compared the performance of the top-performing models with the baseline model.","metadata":{"id":"7df28ac4"}},{"cell_type":"markdown","source":"## 2.2 Architecture Used","metadata":{}},{"cell_type":"markdown","source":"We are going to use the model callled EfficientNet with it's weight when we use ImageNet. We should also stop the Based model from training again and only use the previous weights. The detector was based on EfficientNet,with ImageNet pre-trainedmodel.\n\n- Base Model\n  - The model was fine-tuned with a small learning rate,\n  - Then the extracted features were fed to the decision module. \n\n- Decision Model (Output Layer)\n  - In the decision module, the extracted\n  features are obtained by a fully connected layer with output size 1024, and ReLU activation.\n  - To avoid overfitting, a dropout layer was added before the last fully connected\n  layer. Sigmoid activation is used to return binary classification\n\n  - Participants used the Adam optimizer, with an initial learning rate of 10^-4\n  - The learning rate was reduced by a factor of 0.1 if the validation loss did not decrease after five epochs.\n  - The last layerâ€™s activation function is Sigmoid, while the loss function is\n  Binary Cross-Entropy\n\n\n- The testing batch sizes were 32, 64 and 128. Classification results achieved using various version of EfficientNet (B0, B4, and B5) and various sizes of the input image patch (64, 128) were reported.","metadata":{}},{"cell_type":"code","source":"from IPython.display import Image\nImage('https://i.postimg.cc/sfLF2QRX/image.png')","metadata":{"execution":{"iopub.status.busy":"2023-04-01T00:07:13.238327Z","iopub.execute_input":"2023-04-01T00:07:13.238815Z","iopub.status.idle":"2023-04-01T00:07:13.981152Z","shell.execute_reply.started":"2023-04-01T00:07:13.238771Z","shell.execute_reply":"2023-04-01T00:07:13.979906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color='blue' size='5px'/> Implement in Project<font/>","metadata":{"id":"55a90d9d"}},{"cell_type":"markdown","source":"# 1 Packages","metadata":{"id":"0e3d9080"}},{"cell_type":"code","source":"!pip install keras-cv-attention-models","metadata":{"execution":{"iopub.status.busy":"2023-04-01T00:02:22.896742Z","iopub.execute_input":"2023-04-01T00:02:22.897124Z","iopub.status.idle":"2023-04-01T00:03:06.081464Z","shell.execute_reply.started":"2023-04-01T00:02:22.897093Z","shell.execute_reply":"2023-04-01T00:03:06.080136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"id":"WFjBKbgYQyZ2","execution":{"iopub.status.busy":"2023-04-01T00:03:09.601244Z","iopub.execute_input":"2023-04-01T00:03:09.601715Z","iopub.status.idle":"2023-04-01T00:03:10.169709Z","shell.execute_reply.started":"2023-04-01T00:03:09.601673Z","shell.execute_reply":"2023-04-01T00:03:10.168629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_addons as tfa\nimport tensorflow_datasets as tfds \nfrom kaggle_datasets import KaggleDatasets","metadata":{"id":"KxLNzwk14UNT","execution":{"iopub.status.busy":"2023-04-01T00:03:11.216655Z","iopub.execute_input":"2023-04-01T00:03:11.217168Z","iopub.status.idle":"2023-04-01T00:03:22.190430Z","shell.execute_reply.started":"2023-04-01T00:03:11.217125Z","shell.execute_reply":"2023-04-01T00:03:22.189320Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.layers import Input,Dense, Flatten,Dropout,Conv2D,GlobalAveragePooling2D,MaxPool2D\nfrom keras.models import Sequential\nfrom keras.models import Model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Input, ZeroPadding2D\nfrom tensorflow.keras.applications import EfficientNetB5\nfrom keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.layers import Activation\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import EfficientNetB7\nfrom keras_cv_attention_models import convnext\nfrom keras_cv_attention_models import mobilenetv3","metadata":{"id":"r6PEY9yYQyvr","execution":{"iopub.status.busy":"2023-04-01T00:03:22.192646Z","iopub.execute_input":"2023-04-01T00:03:22.193327Z","iopub.status.idle":"2023-04-01T00:03:22.247363Z","shell.execute_reply.started":"2023-04-01T00:03:22.193283Z","shell.execute_reply":"2023-04-01T00:03:22.246397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from multiprocessing import cpu_count\nfrom tqdm.notebook import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-04-01T00:03:22.248727Z","iopub.execute_input":"2023-04-01T00:03:22.249101Z","iopub.status.idle":"2023-04-01T00:03:22.255338Z","shell.execute_reply.started":"2023-04-01T00:03:22.249065Z","shell.execute_reply":"2023-04-01T00:03:22.253857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings \nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","metadata":{"id":"ec9sVCHTRonN","execution":{"iopub.status.busy":"2023-04-01T00:03:22.257661Z","iopub.execute_input":"2023-04-01T00:03:22.258413Z","iopub.status.idle":"2023-04-01T00:03:22.266794Z","shell.execute_reply.started":"2023-04-01T00:03:22.258377Z","shell.execute_reply":"2023-04-01T00:03:22.265843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport PIL\nimport PIL.Image\nimport pickle \nimport math\nimport cv2\nimport datetime\nimport random\nimport shutil\nimport time\nimport gc\nimport sys\n\nprint(f'Tensorflow Version: {tf.__version__}')\nprint(f'Python Version: {sys.version}')","metadata":{"id":"YFTYUy_CRoql","execution":{"iopub.status.busy":"2023-04-01T00:03:22.268309Z","iopub.execute_input":"2023-04-01T00:03:22.268833Z","iopub.status.idle":"2023-04-01T00:03:22.682552Z","shell.execute_reply.started":"2023-04-01T00:03:22.268783Z","shell.execute_reply":"2023-04-01T00:03:22.681107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2 Explore Data","metadata":{"id":"2077bdf9"}},{"cell_type":"markdown","source":"$$ Train$$","metadata":{"id":"LI-nfSYB4C-9"}},{"cell_type":"code","source":"os.listdir('/kaggle/input/fake-images-detecttion-dataset/Albumentations_data-20230318T161556Z-001/Albumentations_data/Train_Data')\n\n## Define the ImageDataGenerator object\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,              # Rescale pixel values between 0 and 1\n    rotation_range=30,           # Rotate images by up to 30 degrees\n    width_shift_range=0.2,       # Shift images horizontally by up to 20%\n    height_shift_range=0.2,      # Shift images vertically by up to 20%\n    shear_range=0.2,             # Apply shear transformation\n    zoom_range=0.2,              # Zoom in or out by up to 20%\n    horizontal_flip=True,        # Flip images horizontally\n    fill_mode='nearest'          # Fill any gaps created by transformations with the nearest pixel\n)\n\n## Flow from Directory\ntrain_generator=test_datagen.flow_from_directory(\n    '/kaggle/input/fake-images-detecttion-dataset/Albumentations_data-20230318T161556Z-001/Albumentations_data/Train_Data',   # Directory containing images\n    target_size=(160,160),                                            # Resizes images to 160x160 pixels\n    batch_size=32,                                                    # Number of images to include in each batch\n    class_mode='categorical'                                               # Generate binary labels (0 or 1)\n)","metadata":{"id":"UkDHmia-6PLh","execution":{"iopub.status.busy":"2023-03-29T20:58:01.895239Z","iopub.execute_input":"2023-03-29T20:58:01.895607Z","iopub.status.idle":"2023-03-29T20:58:01.901446Z","shell.execute_reply.started":"2023-03-29T20:58:01.895576Z","shell.execute_reply":"2023-03-29T20:58:01.900194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds = tf.keras.utils.image_dataset_from_directory(\n  '/kaggle/input/fake-images-detecttion-dataset/Keras_Data-20230318T161559Z-001/Keras_Data/Train_Data',\n  seed=0,\n  image_size=(160, 160),\n  batch_size=16)","metadata":{"execution":{"iopub.status.busy":"2023-04-01T00:03:27.119090Z","iopub.execute_input":"2023-04-01T00:03:27.119463Z","iopub.status.idle":"2023-04-01T00:04:34.532305Z","shell.execute_reply.started":"2023-04-01T00:03:27.119429Z","shell.execute_reply":"2023-04-01T00:04:34.531244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"$$ Val$$","metadata":{"id":"j7fFuNNR4JW_"}},{"cell_type":"code","source":"os.listdir('/kaggle/input/fake-images-detecttion-dataset/Albumentations_data-20230318T161556Z-001/Albumentations_data/Validation_Data')\n\n## Define the ImageDataGenerator object\nval_datagen = ImageDataGenerator(\n    rescale=1./255,              # Rescale pixel values between 0 and 1\n    rotation_range=30,           # Rotate images by up to 30 degrees\n    width_shift_range=0.2,       # Shift images horizontally by up to 20%\n    height_shift_range=0.2,      # Shift images vertically by up to 20%\n    shear_range=0.2,             # Apply shear transformation\n    zoom_range=0.2,              # Zoom in or out by up to 20%\n    horizontal_flip=True,        # Flip images horizontally\n    fill_mode='nearest'          # Fill any gaps created by transformations with the nearest pixel\n)\n\nval_generator=test_datagen.flow_from_directory(\n    '/kaggle/input/fake-images-detecttion-dataset/Albumentations_data-20230318T161556Z-001/Albumentations_data/Validation_Data',     # Directory containing images\n    target_size=(160,160),                                            # Resizes images to 160x160 pixels\n    batch_size=32,                                                    # Number of images to include in each batch\n    class_mode='categorical'                                               # Generate binary labels (0 or 1)\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_ds = tf.keras.utils.image_dataset_from_directory(\n  '/kaggle/input/fake-images-detecttion-dataset/Keras_Data-20230318T161559Z-001/Keras_Data/Validation_Data',\n  seed=0,\n  image_size=(160, 160),\n  batch_size=8)","metadata":{"execution":{"iopub.status.busy":"2023-04-01T00:04:34.534569Z","iopub.execute_input":"2023-04-01T00:04:34.534979Z","iopub.status.idle":"2023-04-01T00:04:38.915842Z","shell.execute_reply.started":"2023-04-01T00:04:34.534941Z","shell.execute_reply":"2023-04-01T00:04:38.914845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"$$ Test$$","metadata":{}},{"cell_type":"code","source":"os.listdir('/kaggle/input/fake-images-detecttion-dataset/Albumentations_data-20230318T161556Z-001/Albumentations_data/Test_Data')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Define the ImageDataGenerator object\ntest_datagen = ImageDataGenerator(\n    rescale=1./255,              # Rescale pixel values between 0 and 1\n    rotation_range=30,           # Rotate images by up to 30 degrees\n    width_shift_range=0.2,       # Shift images horizontally by up to 20%\n    height_shift_range=0.2,      # Shift images vertically by up to 20%\n    shear_range=0.2,             # Apply shear transformation\n    zoom_range=0.2,              # Zoom in or out by up to 20%\n    horizontal_flip=True,        # Flip images horizontally\n    fill_mode='nearest'          # Fill any gaps created by transformations with the nearest pixel\n)\n\ntest_generator=test_datagen.flow_from_directory(\n    '/kaggle/input/fake-images-detecttion-dataset/Albumentations_data-20230318T161556Z-001/Albumentations_data/Test_Data',    # Directory containing images\n    target_size=(160,160),                                            # Resizes images to 160x160 pixels\n    batch_size=32,                                                    # Number of images to include in each batch\n    class_mode='categorical'                                               # Generate binary labels (0 or 1)\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ds = tf.keras.utils.image_dataset_from_directory(\n  '/kaggle/input/fake-images-detecttion-dataset/Keras_Data-20230318T161559Z-001/Keras_Data/Test_Data',\n  seed=0,\n  image_size=(160, 160),\n  batch_size=8)","metadata":{"execution":{"iopub.status.busy":"2023-04-01T00:04:38.917272Z","iopub.execute_input":"2023-04-01T00:04:38.917635Z","iopub.status.idle":"2023-04-01T00:04:43.285570Z","shell.execute_reply.started":"2023-04-01T00:04:38.917593Z","shell.execute_reply":"2023-04-01T00:04:43.284473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"AutoTune=tf.data.AUTOTUNE\ntrain_ds=train_ds.prefetch(buffer_size=AutoTune)\nval_ds=valid_ds.prefetch(buffer_size=AutoTune)\ntest_ds= test_ds.prefetch(buffer_size=AutoTune)","metadata":{"execution":{"iopub.status.busy":"2023-04-01T01:42:16.778465Z","iopub.status.idle":"2023-04-01T01:42:16.779442Z","shell.execute_reply.started":"2023-04-01T01:42:16.779123Z","shell.execute_reply":"2023-04-01T01:42:16.779155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3 Freature Engineering ","metadata":{"id":"915867c0"}},{"cell_type":"markdown","source":"## 3.1 Preprocessing Data","metadata":{"id":"hRfdXcQetqJH"}},{"cell_type":"markdown","source":"### 3.1.1 Randomize the Input Image","metadata":{}},{"cell_type":"code","source":"# Seed all random number generators\ndef seed_everything(seed=0):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\nseed_everything()","metadata":{"execution":{"iopub.status.busy":"2023-04-01T00:04:43.297372Z","iopub.execute_input":"2023-04-01T00:04:43.297788Z","iopub.status.idle":"2023-04-01T00:04:43.307159Z","shell.execute_reply.started":"2023-04-01T00:04:43.297752Z","shell.execute_reply":"2023-04-01T00:04:43.306259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.1.2 Normalize the Image to be  similar to Image trained on","metadata":{}},{"cell_type":"code","source":"def normalize(image):\n# Repeat channels to create 3 channel images required by pretrained ConvNextV2 models\n# image = tf.repeat(image, repeats=3, axis=3)\n    # Repeat channels to create 3 channel images required by pretrained ConvNextV2 models\n    # image=tf.repeat(image,repeats=3,axis=3)\n    # Cast to float32\n    image=tf.cast(image,tf.float32)\n    #Normalize with respect to ImageNet Mean\n    image=tf.keras.applications.imagenet_utils.preprocess_input(image,mode='torch')\n    return image","metadata":{"execution":{"iopub.status.busy":"2023-04-01T00:04:43.310301Z","iopub.execute_input":"2023-04-01T00:04:43.311154Z","iopub.status.idle":"2023-04-01T00:04:43.318387Z","shell.execute_reply.started":"2023-04-01T00:04:43.311113Z","shell.execute_reply":"2023-04-01T00:04:43.317503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"STRATEGY=tf.distribute.MirroredStrategy()\nN_REPLICAS=STRATEGY.num_replicas_in_sync","metadata":{"execution":{"iopub.status.busy":"2023-04-01T00:04:43.320087Z","iopub.execute_input":"2023-04-01T00:04:43.320497Z","iopub.status.idle":"2023-04-01T00:04:43.644767Z","shell.execute_reply.started":"2023-04-01T00:04:43.320454Z","shell.execute_reply":"2023-04-01T00:04:43.643692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N_REPLICAS","metadata":{"execution":{"iopub.status.busy":"2023-04-01T00:04:43.646684Z","iopub.execute_input":"2023-04-01T00:04:43.647082Z","iopub.status.idle":"2023-04-01T00:04:43.655348Z","shell.execute_reply.started":"2023-04-01T00:04:43.647043Z","shell.execute_reply":"2023-04-01T00:04:43.654078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2 Feature Extraction","metadata":{"id":"P9UtIlEwpJUh"}},{"cell_type":"markdown","source":"During training we will use EfficientNet for that","metadata":{"id":"U4zrDUC-26sj"}},{"cell_type":"markdown","source":"## 3.3 Dimensionality Reduction","metadata":{"id":"E1ewZG9bpPPR"}},{"cell_type":"markdown","source":"We will use Pooling in Training","metadata":{"id":"Vnc3MdBB3BM5"}},{"cell_type":"markdown","source":"# 4 Preprocessing","metadata":{"id":"c0c20235"}},{"cell_type":"markdown","source":"We will split data and normalize it at the same time using ImageGenerator. Adding to that we will introduce some variation. The class will be defined based on class_mode from flow_from_directory fn","metadata":{"id":"be3411c6"}},{"cell_type":"markdown","source":"# 5 Training","metadata":{"id":"530114cb"}},{"cell_type":"markdown","source":"## 5.1 Input Parameters","metadata":{}},{"cell_type":"code","source":"N_EPOCHS=3\nVERBOSE=1\nN_REPLICAS=2\nLR_MAX=5e-6 * N_REPLICAS\nWD_RATIO=1e-5\nN_WARMUP_EPOCHS=0","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.2 Weight Decay Function","metadata":{}},{"cell_type":"code","source":"# Tensorflow Learning Rate Scheduler does not update weight decay, need to do it manually in a custom callback\nclass WeightDecayCallback(tf.keras.callbacks.Callback):\n    def __init__(self, wd_ratio=WD_RATIO):\n        self.step_counter = 0\n        self.wd_ratio = wd_ratio\n    \n    def on_epoch_begin(self, epoch, logs=None):\n        model.optimizer.weight_decay = model.optimizer.learning_rate * self.wd_ratio\n        print(f'learning rate: {model.optimizer.learning_rate.numpy():.2e}, weight decay: {model.optimizer.weight_decay.numpy():.2e}')","metadata":{"execution":{"iopub.status.busy":"2023-04-01T01:36:52.236646Z","iopub.execute_input":"2023-04-01T01:36:52.237401Z","iopub.status.idle":"2023-04-01T01:36:52.244704Z","shell.execute_reply.started":"2023-04-01T01:36:52.237350Z","shell.execute_reply":"2023-04-01T01:36:52.243498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.3 Learning Rate Function","metadata":{}},{"cell_type":"code","source":"# Learning rate scheduler with logaritmic warmup and cosine decay\ndef lrfn(current_step, num_warmup_steps, lr_max, num_cycles=0.50, num_training_steps=N_EPOCHS):\n    \n    if current_step < num_warmup_steps:\n        return lr_max * 0.10 ** (num_warmup_steps - current_step)\n    else:\n        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n\n        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))) * lr_max","metadata":{"execution":{"iopub.status.busy":"2023-04-01T01:37:23.585543Z","iopub.execute_input":"2023-04-01T01:37:23.585955Z","iopub.status.idle":"2023-04-01T01:37:23.593061Z","shell.execute_reply.started":"2023-04-01T01:37:23.585920Z","shell.execute_reply":"2023-04-01T01:37:23.591899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LR_SCHEDULE = [lrfn(step, num_warmup_steps= N_WARMUP_EPOCHS, lr_max=LR_MAX, num_cycles=.5) for step in range(N_EPOCHS)]\nlr_callback=tf.keras.callbacks.LearningRateScheduler(lambda step: LR_SCHEDULE[step],verbose=0)","metadata":{"execution":{"iopub.status.busy":"2023-04-01T01:41:30.535717Z","iopub.execute_input":"2023-04-01T01:41:30.536451Z","iopub.status.idle":"2023-04-01T01:41:30.542055Z","shell.execute_reply.started":"2023-04-01T01:41:30.536415Z","shell.execute_reply":"2023-04-01T01:41:30.540985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.4 EfficientNetB5 & ImageNet Base Model\n","metadata":{"id":"ezUzF-jWiQTe"}},{"cell_type":"code","source":"def get_model():\n    # Verify Mixed Policy settings\n    # Print('f'compute dtype)\n    \n    INPUT_SHAPE=(160,160,3)\n    \n    with STRATEGY.scope():\n        # Set seed for deterministic weights intializations\n        seed_everything()\n        \n        # Input, note the names are equal to directory keys in dataset\n        image=tf.keras.layers.Input(INPUT_SHAPE, name='image',dtype=tf.uint8)\n        \n        \n        # Nomalize Input\n        image_norm=normalize(image)\n        \n        # CNN Prediciton in range [0,1]\n        x = EfficientNetB5(weights='imagenet',include_top=False,input_shape=[160,160,3])(image_norm)\n        \n            ##'imagenet21k-ft1k',imagenet21k-ftlk'\n        \n        \n        \n        # Average Pooling BxhxNxC -> BxC\n        x=GlobalAveragePooling2D()(x)\n        # Dropout \n        x=Dropout(.5)(x) #\n        # Output sigmoid\n        outputs=Dense(1,activation='sigmoid')(x)\n        \n        \n        ## We will use Adam optimizer for fast learning with weight decap\n        optimizer=tfa.optimizers.AdamW(learning_rate=LR_MAX,weight_decay=LR_MAX*WD_RATIO,epsilon=1e-6)\n        \n        ## Loss\n        loss=tf.keras.losses.BinaryCrossentropy(from_logits=False)\n        \n        \n        ## Metrics\n        metrics= [tfa.metrics.F1Score(num_classes=1),\n        tf.keras.metrics.Precision(),\n        tf.keras.metrics.Recall(),\n        tf.keras.metrics.AUC(),\n        tf.keras.metrics.BinaryAccuracy()]\n        \n        model=Model(inputs=image,outputs=outputs)\n        \n        model.compile(optimizer=optimizer,loss=loss,metrics=metrics)\n        \n        return model\n","metadata":{"execution":{"iopub.status.busy":"2023-04-01T00:10:59.474533Z","iopub.execute_input":"2023-04-01T00:10:59.475090Z","iopub.status.idle":"2023-04-01T00:10:59.488409Z","shell.execute_reply.started":"2023-04-01T00:10:59.475044Z","shell.execute_reply":"2023-04-01T00:10:59.487107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LR_SCHEDULE = [lrfn(step, num_warmup_steps= N_WARMUP_EPOCHS, lr_max=LR_MAX, num_cycles=.5) for step in range(N_EPOCHS)]\nlr_callback=tf.keras.callbacks.LearningRateScheduler(lambda step: LR_SCHEDULE[step],verbose=0)\nmodel=get_model()","metadata":{"execution":{"iopub.status.busy":"2023-04-01T00:11:16.203165Z","iopub.execute_input":"2023-04-01T00:11:16.204088Z","iopub.status.idle":"2023-04-01T00:11:31.458138Z","shell.execute_reply.started":"2023-04-01T00:11:16.204037Z","shell.execute_reply":"2023-04-01T00:11:31.456521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history=model.fit(train_ds,\n                  validation_data=valid_ds,\n                  epochs=N_EPOCHS,\n                  verbose=VERBOSE,\n                  callbacks=[lr_callback,\n                             WeightDecayCallback()])","metadata":{"execution":{"iopub.status.busy":"2023-04-01T00:11:32.494316Z","iopub.execute_input":"2023-04-01T00:11:32.494919Z","iopub.status.idle":"2023-04-01T00:55:05.383728Z","shell.execute_reply.started":"2023-04-01T00:11:32.494866Z","shell.execute_reply":"2023-04-01T00:55:05.382903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Base_model = VGG16(input_shape=[160,160,3], weights='imagenet', include_top=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add a Swish activation layer\n#swish_activation = Activation('swish')(Base_model.output)\n\n\n## Add Pooling layer\n#L=GlobalAveragePooling2D()(Base_model.output)\n\n## Add Fully connected Layer\n#L=Dense(1024,activation='relu')(L)\n\n\n\n## Add Dropout to stop overfitting\n#L=Dropout(.5)(L)\n#\n## Output layer\n#output=Dense(2,activation='sigmoid')(L)","metadata":{"id":"sD4h30I2vKI2","execution":{"iopub.status.busy":"2023-03-29T15:28:51.842483Z","iopub.status.idle":"2023-03-29T15:28:51.844346Z","shell.execute_reply.started":"2023-03-29T15:28:51.844029Z","shell.execute_reply":"2023-03-29T15:28:51.844059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#x = Flatten()(Base_model.output)\n#outputs = Dense(2, activation= 'softmax')(x)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T20:58:33.827069Z","iopub.execute_input":"2023-03-29T20:58:33.827763Z","iopub.status.idle":"2023-03-29T20:58:33.849469Z","shell.execute_reply.started":"2023-03-29T20:58:33.827725Z","shell.execute_reply":"2023-03-29T20:58:33.848445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.5 EfficientNetB5 & ImageNet Base Model (Paper Architecture)\n","metadata":{}},{"cell_type":"code","source":"\ndef get_model():\n    # Verify Mixed Policy settings\n    # Print('f'compute dtype)\n    \n    INPUT_SHAPE=(160,160,3)\n    \n    with STRATEGY.scope():\n        # Set seed for deterministic weights intializations\n        seed_everything()\n        \n        # Input, note the names are equal to directory keys in dataset\n        image=tf.keras.layers.Input(INPUT_SHAPE, name='image',dtype=tf.uint8)\n        \n        \n        # Nomalize Input\n        image_norm=normalize(image)\n        \n        # CNN Prediciton in range [0,1]\n        x = EfficientNetB5(weights='imagenet',include_top=False,input_shape=[160,160,3])(image_norm)\n        \n        x = GlobalAveragePooling2D()(x)\n        x = Dense(512, activation='relu')(x)\n        x = Dropout(0.5)(x)\n        x = Dense(128, activation='relu')(x)\n        x = Dropout(0.5)(x)\n\n\n        outputs=Dense(1,activation='sigmoid')(x)\n        \n        \n        ## We will use Adam optimizer for fast learning with weight decap\n        optimizer=tfa.optimizers.AdamW(learning_rate=LR_MAX,weight_decay=LR_MAX*WD_RATIO,epsilon=1e-6)\n        \n        ## Loss\n        loss=tf.keras.losses.BinaryCrossentropy(from_logits=False)\n        \n        \n        ## Metrics\n        metrics= [tfa.metrics.F1Score(num_classes=1),\n        tf.keras.metrics.Precision(),\n        tf.keras.metrics.Recall(),\n        tf.keras.metrics.AUC(),\n        tf.keras.metrics.BinaryAccuracy()]\n        \n        model=Model(inputs=image,outputs=outputs)\n        \n        model.compile(optimizer=optimizer,loss=loss,metrics=metrics)\n        \n        return model\n","metadata":{"execution":{"iopub.status.busy":"2023-04-01T01:40:45.295948Z","iopub.execute_input":"2023-04-01T01:40:45.296878Z","iopub.status.idle":"2023-04-01T01:40:45.310771Z","shell.execute_reply.started":"2023-04-01T01:40:45.296798Z","shell.execute_reply":"2023-04-01T01:40:45.309568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=get_model()\nhistory=model.fit(train_ds,\n                  validation_data=valid_ds,\n                  epochs=N_EPOCHS,\n                  verbose=VERBOSE,\n                  callbacks=[lr_callback,\n                             WeightDecayCallback()])","metadata":{"execution":{"iopub.status.busy":"2023-04-01T02:30:33.638424Z","iopub.execute_input":"2023-04-01T02:30:33.638800Z","iopub.status.idle":"2023-04-01T03:14:47.380473Z","shell.execute_reply.started":"2023-04-01T02:30:33.638767Z","shell.execute_reply":"2023-04-01T03:14:47.379407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.6 EfficientNetB7 & ImageNet Base Model (paper Architecture)\n","metadata":{}},{"cell_type":"code","source":"def get_model():\n    # Verify Mixed Policy settings\n    # Print('f'compute dtype)\n    \n    INPUT_SHAPE=(160,160,3)\n    \n    with STRATEGY.scope():\n        # Set seed for deterministic weights intializations\n        seed_everything()\n        \n        # Input, note the names are equal to directory keys in dataset\n        image=tf.keras.layers.Input(INPUT_SHAPE, name='image',dtype=tf.uint8)\n        \n        \n        # Nomalize Input\n        image_norm=normalize(image)\n        \n        # CNN Prediciton in range [0,1]\n        x = EfficientNetB7(weights='imagenet',include_top=False,input_shape=[160,160,3])(image_norm)\n        \n        x = GlobalAveragePooling2D()(x)\n        x = Dense(1024, activation='relu')(x)\n        x = Dropout(0.5)(x)\n\n\n        outputs=Dense(1,activation='sigmoid')(x)\n        \n        \n        ## We will use Adam optimizer for fast learning with weight decap\n        optimizer=tfa.optimizers.AdamW(learning_rate=LR_MAX,weight_decay=LR_MAX*WD_RATIO,epsilon=1e-6)\n        \n        ## Loss\n        loss=tf.keras.losses.BinaryCrossentropy(from_logits=False)\n        \n        \n        ## Metrics\n        metrics= [tfa.metrics.F1Score(num_classes=1),\n        tf.keras.metrics.Precision(),\n        tf.keras.metrics.Recall(),\n        tf.keras.metrics.AUC(),\n        tf.keras.metrics.BinaryAccuracy()]\n        \n        model=Model(inputs=image,outputs=outputs)\n        \n        model.compile(optimizer=optimizer,loss=loss,metrics=metrics)\n        \n        return model\n","metadata":{"execution":{"iopub.status.busy":"2023-04-01T03:34:34.506769Z","iopub.execute_input":"2023-04-01T03:34:34.507504Z","iopub.status.idle":"2023-04-01T03:34:34.522930Z","shell.execute_reply.started":"2023-04-01T03:34:34.507467Z","shell.execute_reply":"2023-04-01T03:34:34.521865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=get_model()\nhistory=model.fit(train_ds,\n                  validation_data=valid_ds,\n                  epochs=N_EPOCHS,\n                  verbose=VERBOSE,\n                  callbacks=[lr_callback,\n                             WeightDecayCallback()])\nplt.plot(LR_SCHEDULE)\nplt.xlabel('Epoch')\nplt.ylabel('Learning Rate')\nplt.title('Learning Rate Schedule')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-01T03:36:53.845181Z","iopub.execute_input":"2023-04-01T03:36:53.846199Z","iopub.status.idle":"2023-04-01T04:45:24.210855Z","shell.execute_reply.started":"2023-04-01T03:36:53.846143Z","shell.execute_reply":"2023-04-01T04:45:24.209843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get training and validation loss and accuracy from the history object\ntrain_loss = history.history['loss']\nval_loss = history.history['val_loss']\n\n\n# Plot the loss\nplt.figure(figsize=(10, 6))\nplt.plot(train_loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-04-01T04:49:12.868692Z","iopub.execute_input":"2023-04-01T04:49:12.869378Z","iopub.status.idle":"2023-04-01T04:49:13.137683Z","shell.execute_reply.started":"2023-04-01T04:49:12.869340Z","shell.execute_reply":"2023-04-01T04:49:13.136623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6 Prediction","metadata":{"id":"fe6b33a6"}},{"cell_type":"markdown","source":"## 6.1 EfficientNetB7 & ImageNet Base Model\n","metadata":{}},{"cell_type":"code","source":"y_pred= []\ny_test= []\n\nfor image_batch, label_batch in test_ds:\n    # apppend True Labels\n    y_test.append(label_batch)\n    # Compute Predictions\n    preds=model.predict(image_batch)\n    ## Append Preiction labels\n    preds[preds<=.5]=0\n    preds[preds>.5]=1\n    y_pred.append(preds)\n    # convert the true and predicted labels into tensors\ncorrect_labels = tf.concat([item for item in y_test], axis = 0)\npredicted_labels = tf.concat([item for item in y_pred], axis = 0)","metadata":{"execution":{"iopub.status.busy":"2023-04-01T00:57:12.219237Z","iopub.execute_input":"2023-04-01T00:57:12.219883Z","iopub.status.idle":"2023-04-01T01:00:31.977959Z","shell.execute_reply.started":"2023-04-01T00:57:12.219841Z","shell.execute_reply":"2023-04-01T01:00:31.976886Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.2 EfficientNetB5 (Paper Architecture)","metadata":{}},{"cell_type":"code","source":"y_pred= []\ny_test= []\n\nfor image_batch, label_batch in test_ds:\n    # apppend True Labels\n    y_test.append(label_batch)\n    # Compute Predictions\n    preds=model.predict(image_batch)\n    ## Append Preiction labels\n    preds[preds<=.5]=0\n    preds[preds>.5]=1\n    y_pred.append(preds)\n    # convert the true and predicted labels into tensors\ncorrect_labels = tf.concat([item for item in y_test], axis = 0)\npredicted_labels = tf.concat([item for item in y_pred], axis = 0)","metadata":{"execution":{"iopub.status.busy":"2023-04-01T03:18:21.734224Z","iopub.execute_input":"2023-04-01T03:18:21.735215Z","iopub.status.idle":"2023-04-01T03:21:48.379391Z","shell.execute_reply.started":"2023-04-01T03:18:21.735177Z","shell.execute_reply":"2023-04-01T03:21:48.378278Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.3 EfficientNetB7 (Paper Architecture)","metadata":{}},{"cell_type":"code","source":"y_pred= []\ny_test= []\n\nfor image_batch, label_batch in test_ds:\n    # apppend True Labels\n    y_test.append(label_batch)\n    # Compute Predictions\n    preds=model.predict(image_batch)\n    ## Append Preiction labels\n    preds[preds<=.5]=0\n    preds[preds>.5]=1\n    y_pred.append(preds)\n    # convert the true and predicted labels into tensors\ncorrect_labels = tf.concat([item for item in y_test], axis = 0)\npredicted_labels = tf.concat([item for item in y_pred], axis = 0)","metadata":{"execution":{"iopub.status.busy":"2023-04-01T04:50:01.268396Z","iopub.execute_input":"2023-04-01T04:50:01.269250Z","iopub.status.idle":"2023-04-01T04:54:11.558187Z","shell.execute_reply.started":"2023-04-01T04:50:01.269209Z","shell.execute_reply":"2023-04-01T04:54:11.557144Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7 Evaluation","metadata":{"id":"8208ddc4"}},{"cell_type":"markdown","source":"## 7.1 EfficientNetB5","metadata":{}},{"cell_type":"code","source":"model.evaluate(test_ds)","metadata":{"execution":{"iopub.status.busy":"2023-04-01T03:21:48.381802Z","iopub.execute_input":"2023-04-01T03:21:48.382113Z","iopub.status.idle":"2023-04-01T03:23:10.592345Z","shell.execute_reply.started":"2023-04-01T03:21:48.382085Z","shell.execute_reply":"2023-04-01T03:23:10.591312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nclassification_report(correct_labels.numpy(), predicted_labels.numpy())","metadata":{"execution":{"iopub.status.busy":"2023-04-01T03:23:10.593748Z","iopub.execute_input":"2023-04-01T03:23:10.596473Z","iopub.status.idle":"2023-04-01T03:23:10.621756Z","shell.execute_reply.started":"2023-04-01T03:23:10.596426Z","shell.execute_reply":"2023-04-01T03:23:10.620745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save_weights('DF_EfficientB5_90.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7.2 EfficientNetB5(Paper Architecture)","metadata":{}},{"cell_type":"code","source":"model.evaluate(test_ds)","metadata":{"execution":{"iopub.status.busy":"2023-04-01T03:24:37.132088Z","iopub.execute_input":"2023-04-01T03:24:37.132480Z","iopub.status.idle":"2023-04-01T03:25:26.833831Z","shell.execute_reply.started":"2023-04-01T03:24:37.132445Z","shell.execute_reply":"2023-04-01T03:25:26.832809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nclassification_report(correct_labels.numpy(), predicted_labels.numpy())","metadata":{"execution":{"iopub.status.busy":"2023-04-01T03:25:26.835861Z","iopub.execute_input":"2023-04-01T03:25:26.836533Z","iopub.status.idle":"2023-04-01T03:25:26.860887Z","shell.execute_reply.started":"2023-04-01T03:25:26.836493Z","shell.execute_reply":"2023-04-01T03:25:26.859881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save_weights('DF_EfficientB5_88.h5')","metadata":{"execution":{"iopub.status.busy":"2023-04-01T03:28:36.031074Z","iopub.execute_input":"2023-04-01T03:28:36.032105Z","iopub.status.idle":"2023-04-01T03:28:37.174925Z","shell.execute_reply.started":"2023-04-01T03:28:36.032062Z","shell.execute_reply":"2023-04-01T03:28:37.173881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7.3 EfficientNetB7 (Paper Architecture)","metadata":{}},{"cell_type":"code","source":"model.evaluate(test_ds)","metadata":{"execution":{"iopub.status.busy":"2023-04-01T04:54:11.560487Z","iopub.execute_input":"2023-04-01T04:54:11.560885Z","iopub.status.idle":"2023-04-01T04:55:15.129569Z","shell.execute_reply.started":"2023-04-01T04:54:11.560847Z","shell.execute_reply":"2023-04-01T04:55:15.128556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nclassification_report(correct_labels.numpy(), predicted_labels.numpy())","metadata":{"execution":{"iopub.status.busy":"2023-04-01T04:55:15.133079Z","iopub.execute_input":"2023-04-01T04:55:15.133369Z","iopub.status.idle":"2023-04-01T04:55:15.156980Z","shell.execute_reply.started":"2023-04-01T04:55:15.133342Z","shell.execute_reply":"2023-04-01T04:55:15.155761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save_weights('DF_EfficientB7_92.5.h5')","metadata":{"execution":{"iopub.status.busy":"2023-04-01T04:46:54.563302Z","iopub.execute_input":"2023-04-01T04:46:54.563703Z","iopub.status.idle":"2023-04-01T04:46:56.272443Z","shell.execute_reply.started":"2023-04-01T04:46:54.563668Z","shell.execute_reply":"2023-04-01T04:46:56.271311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8 New Information","metadata":{"id":"lElF56Vih33N"}},{"cell_type":"markdown","source":"## 8.1 ImageDataGenerator","metadata":{"id":"-RTSLlVx1HM-"}},{"cell_type":"markdown","source":"In the example below,\n \n- We first define an ImageDataGenerator object datagen with various image transformation options.\n\n- Then we define the directory containing our training images and create a generator train_generator that reads images from the directory and applies the transformations defined in datagen.\n\n- We specify that we want to generate binary labels (0 or 1) using the class_mode parameter.\n\n- Finally, we use train_generator to train our model using the fit method. The fit method will automatically read images from train_generator in batches, apply the specified transformations, and feed them to the model for training.","metadata":{"id":"MTmPz6QU1QzJ"}},{"cell_type":"markdown","source":"- When you use the flow_from_directory method of the ImageDataGenerator class, it reads images from the specified directory, preprocesses them based on the arguments you passed to the ImageDataGenerator, and returns an iterator that yields batches of image arrays and corresponding labels.","metadata":{"id":"An5mvFPB1wdr"}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Define the ImageDataGenerator object\ndatagen = ImageDataGenerator(\n    rescale=1./255,              # Rescale pixel values between 0 and 1\n    rotation_range=30,           # Rotate images by up to 30 degrees\n    width_shift_range=0.2,       # Shift images horizontally by up to 20%\n    height_shift_range=0.2,      # Shift images vertically by up to 20%\n    shear_range=0.2,             # Apply shear transformation\n    zoom_range=0.2,              # Zoom in or out by up to 20%\n    horizontal_flip=True,        # Flip images horizontally\n    fill_mode='nearest'          # Fill any gaps created by transformations with the nearest pixel\n)\n\n# Define the directory containing your images\ntrain_dir = 'path/to/train/directory'\n\n# Create a generator that reads images from the directory and applies the transformations defined above\ntrain_generator = datagen.flow_from_directory(\n    train_dir,                  # Directory containing images\n    target_size=(224, 224),     # Resizes images to 224x224 pixels\n    batch_size=32,              # Number of images to include in each batch\n    class_mode='binary'         # Generate binary labels (0 or 1)\n)\n\n# Train your model using the generator\nmodel.fit(train_generator, epochs=10)","metadata":{"id":"YJfeX7Dk1Knc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8.2 Transfer Learning Basics","metadata":{"id":"yPUlXUfD1luL"}},{"cell_type":"code","source":"## base_model.trainable = False","metadata":{"id":"6hKqrjd-8ctH","execution":{"iopub.status.busy":"2023-03-31T23:25:41.206348Z","iopub.execute_input":"2023-03-31T23:25:41.206750Z","iopub.status.idle":"2023-03-31T23:25:41.211418Z","shell.execute_reply.started":"2023-03-31T23:25:41.206705Z","shell.execute_reply":"2023-03-31T23:25:41.210319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Is a statement that freezes the layers of the base_model and makes them non-trainable during the subsequent training process.\n\n- When a pre-trained model such as EfficientNetB7 is used, the weights of the model have already been trained on a large dataset (in the case of EfficientNetB7, on the ImageNet dataset), and these weights contain valuable information that can be useful for other similar tasks. \n- By freezing the pre-trained layers, we prevent the optimizer from modifying these weights during training, so we can use them as a fixed feature extractor for our specific task.\n","metadata":{"id":"9hjPeGZRh7XN"}},{"cell_type":"code","source":"## include_top=False","metadata":{"id":"Oktf1YdN8Xxu","execution":{"iopub.status.busy":"2023-03-31T23:25:27.191812Z","iopub.execute_input":"2023-03-31T23:25:27.192741Z","iopub.status.idle":"2023-03-31T23:25:27.198023Z","shell.execute_reply.started":"2023-03-31T23:25:27.192704Z","shell.execute_reply":"2023-03-31T23:25:27.196695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- is an argument in the Keras EfficientNetB7 model constructor that specifies whether or not to include the fully connected layer at the top of the network.\n\n- When include_top is set to True, the default, the fully connected layer is included in the model. This layer is responsible for mapping the features learned by the convolutional layers to the specific output classes of the model.\n\n- When include_top is set to False, the fully connected layer is not included in the model.","metadata":{"id":"T502s67diCYl"}},{"cell_type":"code","source":"## model = tf.keras.models.Model(inputs=base_model.input, outputs=output) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Is a statement that creates a new Keras Model object by specifying the inputs and outputs of the model.\n\n- The inputs argument specifies the input tensor(s) of the model. In this case, base_model.input is used as the input tensor, which is the input to the first layer of the pre-trained EfficientNetB7 model.\n\n- The outputs argument specifies the output tensor(s) of the model. In this case, output is used as the output tensor, which is the output of the custom output layer that we added.\n","metadata":{"id":"se3nFO02jjiP"}},{"cell_type":"markdown","source":"## 8.3 ModelCheckPoint","metadata":{"id":"8VVGYZrbEtC7"}},{"cell_type":"markdown","source":"ModelCheckpoint is a callback in Keras that allows you to save the weights of your model at specified intervals during training. \n- This is useful because it allows you to monitor the progress of your model over time and, in the event that your training is interrupted or crashes, you can resume training from the last saved checkpoint rather than starting over from scratch.","metadata":{"id":"zJz1DtUKEwWr"}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint\n\n# Define the file path for the saved model\nfilepath = \"model_weights.h5\"\n\n# Define the ModelCheckpoint callback\ncheckpoint = ModelCheckpoint(\n    filepath,                   # File path for the saved model\n    monitor='val_loss',         # Quantity to monitor (e.g. validation loss)\n    save_best_only=True,        # Save only the best model based on the monitored quantity\n    mode='min',                 # Minimize the monitored quantity (e.g. validation loss)\n    verbose=1                   # Show progress updates\n)\n\n# Train the model and specify ModelCheckpoint as a callback\nhistory = model.fit(\n    train_generator,\n    epochs=10,\n    validation_data=val_generator,\n    callbacks=[checkpoint]\n)","metadata":{"id":"egOHPG_QE5YT","execution":{"iopub.status.busy":"2023-03-31T23:27:17.707302Z","iopub.execute_input":"2023-03-31T23:27:17.707728Z","iopub.status.idle":"2023-03-31T23:27:20.148089Z","shell.execute_reply.started":"2023-03-31T23:27:17.707693Z","shell.execute_reply":"2023-03-31T23:27:20.146332Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"During training, ModelCheckpoint will save the model to filepath whenever the monitored quantity meets the specified conditions. By default, ModelCheckpoint will save the entire model, including the architecture and optimizer state, but you can also choose to save only the weights or only the architecture by passing in additional parameters to the ModelCheckpoint constructor","metadata":{}},{"cell_type":"markdown","source":"We then train our model using the fit method, and include checkpoint as a callback by passing it in as an element of a list of callbacks.\n- In this example, we define the file path for the saved model as filepath. We then define the ModelCheckpoint callback by passing in various parameters:\n\n- filepath: the file path for the saved model\nmonitor: the quantity to monitor during training. \n- In this example, we monitor the validation loss using 'val_loss'.\n- save_best_only: if True, the callback will only save the best model based on the monitored quantity. In this example, we only save the model if the validation loss is the lowest so far.\n- mode: specifies whether to minimize or maximize the monitored quantity. In this example, we want to minimize the validation loss, so we set mode='min'.\n- verbose: specifies how much progress information to display during training.\n\n\n.","metadata":{"id":"DjFqnDXjE9sI"}},{"cell_type":"markdown","source":"## 8.3.1 CheckPoint in Project","metadata":{}},{"cell_type":"code","source":"from datetime import datetime\nfrom keras.callbacks import ModelCheckpoint\n\ncheckpoint = ModelCheckpoint(filepath='DF_Best_model.h5',\n                             verbose=1,\n                             save_best_only=True,\n                             monitor='val_loss'\n                             )\n\n\n\nstart = datetime.now()\n\nDF_model_history = DF_model.fit(\n    train_generator,\n    validation_data= val_generator,\n    epochs=10,\n    verbose=2\n    )\n\nduration = datetime.now() - start\nprint(\"training completed in time: \", duration)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8.4 Class In python\n","metadata":{}},{"cell_type":"markdown","source":"In object-oriented programming, a class is a blueprint or a template for creating objects that define a set of attributes and methods that the objects can have. \n- Attributes are the properties of an object,\n- Methods are the functions or operations that the object can perform.\n- A constructor in Python is a special method within a class \n    - It is called when an object of that class is instantiated (created). \n    - The constructor method is used to initialize the attributes of an object.\n    - The constructor is used to provide default values for the object's attributes or to accept values that are passed as arguments during the object creation process. \n    - In Python, the constructor method is denoted by the __init__() method. When an object is created, this method is automatically called, and any parameters passed to the object are passed to the constructor as arguments.\n","metadata":{}},{"cell_type":"markdown","source":"- A class can be thought of as a user-defined data type that encapsulates both data and functions.\n- By defining a class, we can create objects of that class, which can be used to store data and perform operations on that data.","metadata":{}},{"cell_type":"markdown","source":"To create a class in Python,:\n- First, you can use the class keyword followed by the name of the class, starting with an uppercase letter. \n- We will use a constructor (__init__) to intialize two arguments\n- We will create BankAccount, which is a class that has an account_number and balance attribute, as well as methods to deposit, withdraw, and get the account balance.\n- We will create an instance of that class (Input some variables)","metadata":{}},{"cell_type":"code","source":"class BankAccount:\n    \n    def __init__(self, account_number, balance):\n        self.account_number = account_number\n        self.balance = balance\n        \n    def deposit(self, amount):\n        self.balance += amount\n        print(f\"Deposited {amount} into account {self.account_number}\")\n        \n    def withdraw(self, amount):\n        if amount > self.balance:\n            print(\"Insufficient funds\")\n        else:\n            self.balance -= amount\n            print(f\"Withdrew {amount} from account {self.account_number}\")\n            \n    def get_balance(self):\n        print(f\"Account {self.account_number} balance: {self.balance}\")","metadata":{"execution":{"iopub.status.busy":"2023-03-31T23:54:46.127916Z","iopub.execute_input":"2023-03-31T23:54:46.128886Z","iopub.status.idle":"2023-03-31T23:54:46.136658Z","shell.execute_reply.started":"2023-03-31T23:54:46.128822Z","shell.execute_reply":"2023-03-31T23:54:46.135574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"account1 = BankAccount(\"12345\", 1000)","metadata":{"execution":{"iopub.status.busy":"2023-03-31T23:54:47.308889Z","iopub.execute_input":"2023-03-31T23:54:47.309259Z","iopub.status.idle":"2023-03-31T23:54:47.316685Z","shell.execute_reply.started":"2023-03-31T23:54:47.309227Z","shell.execute_reply":"2023-03-31T23:54:47.315529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"account1.deposit(500)\naccount1.withdraw(200)\naccount1.get_balance()","metadata":{"execution":{"iopub.status.busy":"2023-03-31T23:55:01.288477Z","iopub.execute_input":"2023-03-31T23:55:01.288851Z","iopub.status.idle":"2023-03-31T23:55:01.296603Z","shell.execute_reply.started":"2023-03-31T23:55:01.288799Z","shell.execute_reply":"2023-03-31T23:55:01.295529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8.5 Decay In Weights","metadata":{}},{"cell_type":"markdown","source":"Weight decay is a regularization technique used in machine learning to prevent overfitting. It works by adding a penalty term to the loss function of the model during training, which encourages the model to have smaller weights. \n- This penalty term is proportional to the L2 norm of the weights, so it is also known as L2 regularization.\n\n- This helps to prevent overfitting by reducing the complexity of the model and making it less sensitive to noise in the training data.","metadata":{}},{"cell_type":"code","source":"class WeightDecayCallback(tf.keras.callbacks.Callback):\n    def __init__(self, wd_ratio=WD_RATIO):\n        self.step_counter = 0\n        self.wd_ratio = wd_ratio\n    \n    def on_epoch_begin(self, epoch, logs=None):\n        model.optimizer.weight_decay = model.optimizer.learning_rate * self.wd_ratio\n        print(f'learning rate: {model.optimizer.learning_rate.numpy():.2e}, weight decay: {model.optimizer.weight_decay.numpy():.2e}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The on_epoch_begin method: The method calculates the weight decay value based on the current learning rate and the wd_ratio, and sets it in the optimizer. It also prints the current learning rate and weight decay values.\n- This callback function is called at the beginning of each epoch during training.\n- It takes the current epoch number and a logs dictionary as arguments (which are provided by Keras). \n- By setting the default value of logs to None, the callback can handle cases where the metrics dictionary is not passed or is empty. This ensures that the callback will not raise an error due to missing data.\n","metadata":{}},{"cell_type":"markdown","source":"## 8.6 Learning Rate Schedules","metadata":{}},{"cell_type":"markdown","source":"The learning rate is the step size that the optimizer takes to update the weights during training, and it determines how fast or slow the model learns.\n- If the learning rate is too high, the model might overshoot the optimal weights and fail to converge\n- If the learning rate is too low, the model might take too long to converge and get stuck in a suboptimal solution.","metadata":{}},{"cell_type":"markdown","source":"### 8.6.1 Learning Rate Schedules ","metadata":{}},{"cell_type":"markdown","source":"Learning rate schedules provide a way to adjust the learning rate during training to find the right balance between convergence speed and accuracy. \n- For example, we can start with a high learning rate to make the model converge quickly, and then reduce it gradually to fine-tune the weights and improve the accuracy. \n- We can also use more complex schedules such as:\n    - cosine annealing,\n    - step decay, \n    - adaptive methods such as Adam that adjust the learning rate based on the gradient and other factors.","metadata":{}},{"cell_type":"markdown","source":"### 8.6.2 Example ","metadata":{}},{"cell_type":"markdown","source":"- We will use warmup period at the beginning \n    - Warmup period is an initial phase in the training process where the learning rate is gradually increased from a small value to a larger value over a certain number of steps or epochs.\n    - The purpose of this warmup period is to allow the model to stabilize and adjust to the new learning rate before the learning rate is increased to a larger value. \n    - This can help prevent the model from becoming unstable or overfitting during the early stages of training, especially when using high learning rates.\n\n- We will use cosine decay adding to with number of cycles\n    -  if NUM_CYCLES is set to 0.5, the learning rate will perform a half cycle of cosine decay from the maximum learning rate to 0, and then increase back to the maximum learning rate in the second half cycle.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport math\n\ndef lr_schedule(epoch, lr, num_epochs=N_EPOCHS, num_cycles=0.5, lr_max=LR_MAX, lr_min=LR_MIN):\n    progress = epoch / num_epochs\n    cosine_decay = 0.5 * (1 + math.cos(math.pi * progress * num_cycles))\n    decayed_lr = (lr_max - lr_min) * cosine_decay + lr_min\n    return decayed_lr / math.log(10) # Apply logarithmic scaling to learning rate\n\ncallbacks = [\n    tf.keras.callbacks.LearningRateScheduler(lr_schedule),\n    # Other callbacks...\n]\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(),\n              loss='binary_crossentropy',\n              metrics=[tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.AUC()])\n\nmodel.fit(train_dataset, epochs=N_EPOCHS, callbacks=callbacks)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Learning rate scheduler with logaritmic warmup and cosine decay\ndef lrfn(current_step, num_warmup_steps, lr_max, num_cycles=0.50, num_training_steps=N_EPOCHS):\n    \n    if current_step < num_warmup_steps:\n        return lr_max * 0.10 ** (num_warmup_steps - current_step)\n    else:\n        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n\n        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))) * lr_max","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LR_SCHEDULE = [lrfn(step, num_warmup_steps= N_WARMUP_EPOCHS, lr_max=LR_MAX, num_cycles=.5) for step in range(N_EPOCHS)]\nlr_callback=tf.keras.callbacks.LearningRateScheduler(lambda step: LR_SCHEDULE[step],verbose=0)\nmodel=get_model()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 8.6.3 Plot LR during Trainig","metadata":{}},{"cell_type":"code","source":"# define the learning rate schedule\ndef lr_schedule(epoch):\n    lr = # calculate the learning rate based on the epoch\n    return lr\n\n# create a learning rate callback\nlr_callback = tf.keras.callbacks.LearningRateScheduler(lr_schedule)\n\n# define the model and compile it with your chosen optimizer and loss function\n\n# train the model with the learning rate callback\nhistory = model.fit(train_data, train_labels, epochs=num_epochs, callbacks=[lr_callback])\n\n# plot the learning rate values for each epoch\nlrs = [lr_schedule(epoch) for epoch in range(num_epochs)]\nplt.plot(lrs)\nplt.xlabel('Epoch')\nplt.ylabel('Learning Rate')\nplt.title('Learning Rate Schedule')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8.7 Cast","metadata":{}},{"cell_type":"markdown","source":"Casting is the process of converting a tensor from one data type to another. It can be useful in various scenarios, such as:\n\n- Ensuring consistency of data types across different parts of the code (e.g., model inputs, outputs, and variables).\n- Improving numerical stability or reducing memory usage by using lower-precision data types (e.g., tf.float16 or tf.bfloat16).\n- Converting data types to match the requirements of a specific operation or model architecture (e.g., some operations only accept certain data types as input).\n","metadata":{}},{"cell_type":"markdown","source":"## 8.8 Normalize Image","metadata":{}},{"cell_type":"markdown","source":"We will need to turn the image to a similar shape to the original image that we had in transfer learning\n- We will cast image to float 32 \n- We will normalize the data based on the mode 'torch', which means the image is normalized using the mean and standard deviation values used by PyTorch models trained on ImageNe\n- EfficientNetB7 is a model from google but Pytorch bought it too","metadata":{}},{"cell_type":"code","source":"def normalize(image):\n    # Cast to float32\n    image=tf.cast(image,tf.float32)\n    #Normalize with respect to ImageNet Mean\n    image=tf.keras.applications.imagenet_utils.preprocess_input(image,mode='torch')\n    return image","metadata":{},"execution_count":null,"outputs":[]}]}